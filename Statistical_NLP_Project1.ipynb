{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Statistical NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is probably the most popular task that you would deal with in real life.\n",
    "Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the\n",
    "information about the writer without knowing about him/her.\n",
    "We are going to create a classifier that predicts multiple features of the author of a given text.\n",
    "We have designed it as a Multilabel classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset :\n",
    "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n",
    "\n",
    "Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
    "\n",
    "All bloggers included in the corpus fall into one of three age groups:\n",
    "\n",
    "8240 \"10s\" blogs (ages 13-17),\n",
    "8086 \"20s\" blogs(ages 23-27)\n",
    "2994 \"30s\" blogs (ages 33-47).\n",
    "For each age group there are an equal number of male and female bloggers.\n",
    "\n",
    "Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n",
    "\n",
    "Link for dataset is \n",
    "https://www.kaggle.com/rtatman/blog-authorship-corpus/  \n",
    "File to be downloaded is blog-authorship-corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.  Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mansvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything, Korea is a country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "8  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "9  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5               I had an interesting conversation...  \n",
       "6               Somehow Coca-Cola has a way of su...  \n",
       "7               If anything, Korea is a country o...  \n",
       "8               Take a read of this news article ...  \n",
       "9               I surf the English news sites a l...  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"blog-authorship-corpus\\\\blogtext.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      "id        681284 non-null int64\n",
      "gender    681284 non-null object\n",
      "age       681284 non-null int64\n",
      "topic     681284 non-null object\n",
      "sign      681284 non-null object\n",
      "date      681284 non-null object\n",
      "text      681284 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "date      0\n",
       "sign      0\n",
       "topic     0\n",
       "age       0\n",
       "gender    0\n",
       "id        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.info())\n",
    "data.isnull().sum().sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Taking 5000 records as sample\n",
    "data = data.sample(5000)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         '"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:5000]\n",
    "print(data.shape)\n",
    "data[\"text\"].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.  Preprocess rows of the “text” column (7.5 points)\n",
    "<br>a. Remove unwanted characters\n",
    "<br>b. Convert text to lowercase\n",
    "<br>c. Remove unwanted spaces\n",
    "<br>d. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace('[^A-Za-z]',' ')\n",
    "data['text'] = data['text'].str.lower()\n",
    "data[\"text\"] = data[\"text\"].str.strip()\n",
    "data[\"text\"] = data[\"text\"].str.split()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def removestopwords(y):   # Function definition\n",
    " stopwordremoved = [w for w in y if w not in stop]\n",
    " return(\" \".join(stopwordremoved)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text column size : 5000\n"
     ]
    }
   ],
   "source": [
    "text_column_size = data[\"text\"].size\n",
    "print(\"text column size :\", text_column_size)\n",
    "\n",
    "# Initialize an empty list to hold the text after stop word removal\n",
    "data_cleaner = []\n",
    "\n",
    "# Loop over each text\n",
    "for i in range( 0, text_column_size):\n",
    "    data_cleaner.append(removestopwords(data[\"text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interesting conversation dad morning talking koreans put money invariably lot real estate cash cash would include short term investments one year well savings accounts reason real estate makes money lot money seen surveys seoul real estate rising per year long stretches even taking account crisis referred imf crisis although imf bailed korea compare korean corporate bonds fell modestly recovered local stock market represented kospi version dow jones index gone appreciably high points points see urllink link see real estate makes sense back conversation noted real big elite real estate investor billion usd see urllink converter properties dad seemed little flabbergasted heck need million dollars need much retire maybe lot risk take real estate south korean asset example north toots horn louder make move country usd worth cents also denominated imf crisis dropped vis vis usd also make bad investment fall victim scam latest urllink good morning city project toast saw lady tv lost everything comment tears know like go rich person beggar one day one saber rattling north korea weak exchange rate little nest egg could almost wiped government almost zero help unemployed disabled otherwise disenfranchised workers role family important money help family go first thus idea koreans go things investing different one apartment urllink jeonse system supports well see usd apartment rent two systems use korea neither western ones except rare circumstances renter signs year contract deposits half market value usd owner monthly rent paid owner invest korean treasury bills per year monthly rent return end term usd returned renter renter signs year year contract deposits market value property usd plus monthly rent month cases value property increases decreases jeonse need topped partially refunded course using usd save key help foreigners reference better thus buy place turn around rent get like buy another place whatever since mortgages korea kind cash society although home equity lines credit system bit different key course real estate prices keep going'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaner[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data_cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mansvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemm = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    return(\" \".join(lemm)) \n",
    "\n",
    "data[\"text\"] = data.text.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
    "<br>a. Label columns to merge: “gender”, “age”, “topic”, “sign”\n",
    "<br>b. After completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found page mb pdf file wait untill team l...</td>\n",
       "      <td>male,15,Student,Leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team member drewes van der laag urllink mail r...</td>\n",
       "      <td>male,15,Student,Leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>male,15,Student,Leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>male,15,Student,Leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture url popups mean s...</td>\n",
       "      <td>male,33,InvestmentBanking,Aquarius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found page mb pdf file wait untill team l...   \n",
       "1  team member drewes van der laag urllink mail r...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture url popups mean s...   \n",
       "\n",
       "                               labels  \n",
       "0                 male,15,Student,Leo  \n",
       "1                 male,15,Student,Leo  \n",
       "2                 male,15,Student,Leo  \n",
       "3                 male,15,Student,Leo  \n",
       "4  male,33,InvestmentBanking,Aquarius  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['age'] = data['age'].astype(str)\n",
    "data['labels'] = data[['gender','age','topic','sign']].apply(lambda x: ','.join(x), axis = 1) \n",
    "data_merged = data.drop(labels = ['date','gender', 'age','topic','sign','id'], axis = 1)\n",
    "data_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Separate features and labels, and split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3350,), (1650,), (3350,), (1650,))"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_merged['text']\n",
    "y = data_merged['labels'].str.lower()\n",
    "labels = data_merged['labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 143)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Vectorize the features \n",
    "<br>a. Create a Bag of Words using count vectorizer\n",
    "<br>i. Use ngram_range=(1, 2)\n",
    "<br>ii. Vectorize training and testing features\n",
    "<br>b. Print the term-document matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape & sample (3350, 27104)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x27104 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 68 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df = 2,ngram_range = (1,2),stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(\"X_train shape & sample\",X_train.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "Create a dictionary to get the count of every label i.e. the key will be label name and value will  be the total count of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 42,\n",
       " '15': 1,\n",
       " 'student': 54,\n",
       " 'leo': 39,\n",
       " '33': 9,\n",
       " 'investmentbanking': 37,\n",
       " 'aquarius': 21,\n",
       " 'female': 33,\n",
       " '14': 0,\n",
       " 'indunk': 35,\n",
       " 'aries': 22,\n",
       " '25': 6,\n",
       " 'capricorn': 28,\n",
       " '17': 3,\n",
       " 'gemini': 34,\n",
       " '23': 4,\n",
       " 'non': 45,\n",
       " 'profit': 47,\n",
       " 'cancer': 27,\n",
       " 'banking': 25,\n",
       " '37': 13,\n",
       " 'sagittarius': 50,\n",
       " '26': 7,\n",
       " '24': 5,\n",
       " 'scorpio': 52,\n",
       " '27': 8,\n",
       " 'education': 31,\n",
       " '45': 18,\n",
       " 'engineering': 32,\n",
       " 'libra': 40,\n",
       " 'science': 51,\n",
       " '34': 10,\n",
       " '41': 15,\n",
       " 'communications': 29,\n",
       " 'media': 43,\n",
       " 'businessservices': 26,\n",
       " 'sports': 53,\n",
       " 'recreation': 48,\n",
       " 'virgo': 57,\n",
       " 'taurus': 55,\n",
       " 'arts': 23,\n",
       " 'pisces': 46,\n",
       " '44': 17,\n",
       " '16': 2,\n",
       " 'internet': 36,\n",
       " 'museums': 44,\n",
       " 'libraries': 41,\n",
       " 'accounting': 20,\n",
       " '39': 14,\n",
       " '35': 11,\n",
       " 'technology': 56,\n",
       " '36': 12,\n",
       " 'law': 38,\n",
       " '46': 19,\n",
       " 'consulting': 30,\n",
       " 'automotive': 24,\n",
       " '42': 16,\n",
       " 'religion': 49}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_labels = CountVectorizer(min_df = 1,ngram_range = (1,1),stop_words = \"english\")\n",
    "labels_vector = vectorizer_labels.fit_transform(labels)\n",
    "vectorizer_labels.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the vocab keys. These set of labels will be used as classes in multilabelbinariser\n",
    "l_classes = []  \n",
    "for k in vectorizer_labels.vocabulary_.keys():\n",
    "    l_classes.append(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Transform the labels - \n",
    "<br>As we have noticed before, in this task each example can have multiple tags. To deal with\n",
    "such kind of prediction, we need to transform labels in a binary form and the prediction will be\n",
    "a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
    "<br>a. Convert your train and test labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes = l_classes)\n",
    "labels = [[\"\".join(re.findall(\"\\w\",f)) for f in lst] for lst in [s.split(\",\") for s in labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=['male', '15', 'student', 'leo', '33',\n",
       "                             'investmentbanking', 'aquarius', 'female', '14',\n",
       "                             'indunk', 'aries', '25', 'capricorn', '17',\n",
       "                             'gemini', '23', 'non', 'profit', 'cancer',\n",
       "                             'banking', '37', 'sagittarius', '26', '24',\n",
       "                             'scorpio', '27', 'education', '45', 'engineering',\n",
       "                             'libra', ...],\n",
       "                    sparse_output=False)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_trans = mlb.fit(labels) # transforming entire set of lables\n",
    "labels_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) ['communicationsmedia', 'museumslibraries', 'nonprofit', 'sportsrecreation'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:987: UserWarning: unknown class(es) ['communicationsmedia', 'nonprofit', 'sportsrecreation'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "y_train = [[\"\".join(re.findall(\"\\w\",f)) for f in lst] for lst in [s.split(\",\") for s in y_train]]\n",
    "y_train_tr = mlb.transform(y_train)\n",
    "y_test = [[\"\".join(re.findall(\"\\w\",f)) for f in lst] for lst in [s.split(\",\") for s in y_test]]\n",
    "y_test_tr = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', '15', 'student', 'leo', '33', 'investmentbanking',\n",
       "       'aquarius', 'female', '14', 'indunk', 'aries', '25', 'capricorn',\n",
       "       '17', 'gemini', '23', 'non', 'profit', 'cancer', 'banking', '37',\n",
       "       'sagittarius', '26', '24', 'scorpio', '27', 'education', '45',\n",
       "       'engineering', 'libra', 'science', '34', '41', 'communications',\n",
       "       'media', 'businessservices', 'sports', 'recreation', 'virgo',\n",
       "       'taurus', 'arts', 'pisces', '44', '16', 'internet', 'museums',\n",
       "       'libraries', 'accounting', '39', '35', 'technology', '36', 'law',\n",
       "       '46', 'consulting', 'automotive', '42', 'religion'], dtype=object)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['male', '35', 'technology', 'aries']"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test_tr[10])\n",
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8:\n",
    "\n",
    "In this task, we suggest using the One-vs-Rest approach, which is implemented in  OneVsRestClassifier​ class. In this approach k classifiers (= number of tags) are trained. As a  basic classifier, use ​LogisticRegression​. It is one of the simplest methods, but often it  performs good enough in text classification tasks. It might take some time because the  number of classifiers to train is large.  \n",
    "\n",
    "a. Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on  every label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver = 'lbfgs', max_iter = 1000)  # initiating the classifier\n",
    "clf = OneVsRestClassifier(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9:\n",
    "\n",
    "Fit the classifier, make predictions and get the accuracy\n",
    "\n",
    "a. Print the following  \n",
    "        i. Accuracy score  \n",
    "        ii. F1 score  \n",
    "        iii. Average precision score  \n",
    "        iv. Average recall score \n",
    " \n",
    "Tip: Make sure you are familiar with all of them. How would you expect the  things to work for the multi-label scenario? Read about micro/macro/weighted  averaging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 16 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 17 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 33 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 34 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 36 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 37 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 45 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "d:\\anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\multiclass.py:75: UserWarning: Label not 46 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=1000,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9561194029850746\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy:\",clf.score(X_train,y_train_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.5315151515151515\n",
      "F1: 0.7432240847302143\n",
      "F1_macro: 0.25418706963707105\n",
      "Precision: 0.8188166115398751\n",
      "Precision_macro: 0.42299519494609034\n",
      "Recall: 0.6804092227821041\n",
      "Recall_macro: 0.2056911276465752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, accuracy_score, recall_score, precision_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "print(\"Test Accuracy:\" + str(accuracy_score(y_test_tr, Y_pred)))\n",
    "print(\"F1: \" + str(f1_score(y_test_tr, Y_pred, average='micro')))\n",
    "print(\"F1_macro: \" + str(f1_score(y_test_tr, Y_pred, average='macro')))\n",
    "print(\"Precision: \" + str(precision_score(y_test_tr, Y_pred, average='micro')))\n",
    "print(\"Precision_macro: \" + str(precision_score(y_test_tr, Y_pred, average='macro')))\n",
    "print(\"Recall: \" + str(recall_score(y_test_tr, Y_pred, average='micro')))\n",
    "print(\"Recall_macro: \" + str(recall_score(y_test_tr, Y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "Print true label and predicted label for any five examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_inv = mlb.inverse_transform(Y_pred)   # inverse transforming predited label data\n",
    "Y_test_trans_inv =  mlb.inverse_transform(y_test_tr) # inverse transforming original test label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - predicted : ('male', 'aries', '35', 'technology')\n",
      "Example 1 - Actual : ('male', 'aries', '35', 'technology')\n",
      "Example 1 - Actual_before mlb transformation : ['male', '35', 'technology', 'aries']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1 - predicted :\",Y_pred_inv[0])\n",
    "print(\"Example 1 - Actual :\",Y_test_trans_inv[0])\n",
    "print(\"Example 1 - Actual_before mlb transformation :\",y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2 - predicted : ('male',)\n",
      "Example 2 - Actual : ('leo', 'female', 'indunk', '16')\n",
      "Example 2 - Actual_before mlb transformation : ['female', '16', 'indunk', 'leo']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 2 - predicted :\",Y_pred_inv[30])\n",
    "print(\"Example 2 - Actual :\",Y_test_trans_inv[30])\n",
    "print(\"Example 2 - Actual_before mlb transformation :\",y_test[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3 - predicted : ('female', 'indunk', 'scorpio')\n",
      "Example 3 - Actual : ('female', 'indunk', '24', 'scorpio')\n",
      "Example 3 - Actual_before mlb transformation : ['female', '24', 'indunk', 'scorpio']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 3 - predicted :\",Y_pred_inv[39])\n",
    "print(\"Example 3 - Actual :\",Y_test_trans_inv[39])\n",
    "print(\"Example 3 - Actual_before mlb transformation :\",y_test[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4 - predicted : ('male', 'leo', 'indunk', '26')\n",
      "Example 4 - Actual : ('male', 'leo', 'indunk', '26')\n",
      "Example 4 - Actual_before mlb transformation : ['male', '26', 'indunk', 'leo']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 4 - predicted :\",Y_pred_inv[300])\n",
    "print(\"Example 4 - Actual :\",Y_test_trans_inv[300])\n",
    "print(\"Example 4 - Actual_before mlb transformation :\",y_test[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5 - predicted : ('female', 'indunk', 'pisces', '36')\n",
      "Example 5 - Actual : ('female', 'indunk', 'pisces', '36')\n",
      "Example 5 - Actual_before mlb transformation : ['male', '35', 'technology', 'aries']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 5 - predicted :\",Y_pred_inv[89])\n",
    "print(\"Example 5 - Actual :\",Y_test_trans_inv[89])\n",
    "print(\"Example 5 - Actual_before mlb transformation :\",y_test[892])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "- Tried this model with 50k, 20k and 3k data also. Could not get accuracy over 53%\n",
    "- For this test, we ran it with 5000 data points.\n",
    "- The testing accuracy of the model is not very good, just close to 53%. Some more iterations and model tuning exercise should be conducted to improve it.\n",
    "- This is also reflected from the above True and Predicted labels. We couldn't get prediction for all classes and these classes are also not predicted correctly.\n",
    "- Lemmatization is used as an additional step in the pre processing, still it is not impacting model generalisation.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
