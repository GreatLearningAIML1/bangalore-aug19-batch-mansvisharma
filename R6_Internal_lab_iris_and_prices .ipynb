{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUZjPnVXGz0Z"
   },
   "source": [
    "# The Iris Dataset\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMbmpriavLE9"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu8bUU__oa7h"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLz1Ckvfvn6D"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWrzVTLOvn6M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uYeJgkNuXNC"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcASNsewsfQX"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-vVQBBqg7DI"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE0EDKvQhEIe"
   },
   "source": [
    "### Import dataset\n",
    "- Import iris dataset\n",
    "- Import the dataset using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOOWpD26Haq3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.1, 3.5, 1.4, 0.2]),\n",
       " array([4.9, 3. , 1.4, 0.2]),\n",
       " array([4.7, 3.2, 1.3, 0.2]),\n",
       " array([4.6, 3.1, 1.5, 0.2]),\n",
       " array([5. , 3.6, 1.4, 0.2]),\n",
       " array([5.4, 3.9, 1.7, 0.4]),\n",
       " array([4.6, 3.4, 1.4, 0.3]),\n",
       " array([5. , 3.4, 1.5, 0.2]),\n",
       " array([4.4, 2.9, 1.4, 0.2]),\n",
       " array([4.9, 3.1, 1.5, 0.1]),\n",
       " array([5.4, 3.7, 1.5, 0.2]),\n",
       " array([4.8, 3.4, 1.6, 0.2]),\n",
       " array([4.8, 3. , 1.4, 0.1]),\n",
       " array([4.3, 3. , 1.1, 0.1]),\n",
       " array([5.8, 4. , 1.2, 0.2]),\n",
       " array([5.7, 4.4, 1.5, 0.4]),\n",
       " array([5.4, 3.9, 1.3, 0.4]),\n",
       " array([5.1, 3.5, 1.4, 0.3]),\n",
       " array([5.7, 3.8, 1.7, 0.3]),\n",
       " array([5.1, 3.8, 1.5, 0.3]),\n",
       " array([5.4, 3.4, 1.7, 0.2]),\n",
       " array([5.1, 3.7, 1.5, 0.4]),\n",
       " array([4.6, 3.6, 1. , 0.2]),\n",
       " array([5.1, 3.3, 1.7, 0.5]),\n",
       " array([4.8, 3.4, 1.9, 0.2]),\n",
       " array([5. , 3. , 1.6, 0.2]),\n",
       " array([5. , 3.4, 1.6, 0.4]),\n",
       " array([5.2, 3.5, 1.5, 0.2]),\n",
       " array([5.2, 3.4, 1.4, 0.2]),\n",
       " array([4.7, 3.2, 1.6, 0.2]),\n",
       " array([4.8, 3.1, 1.6, 0.2]),\n",
       " array([5.4, 3.4, 1.5, 0.4]),\n",
       " array([5.2, 4.1, 1.5, 0.1]),\n",
       " array([5.5, 4.2, 1.4, 0.2]),\n",
       " array([4.9, 3.1, 1.5, 0.2]),\n",
       " array([5. , 3.2, 1.2, 0.2]),\n",
       " array([5.5, 3.5, 1.3, 0.2]),\n",
       " array([4.9, 3.6, 1.4, 0.1]),\n",
       " array([4.4, 3. , 1.3, 0.2]),\n",
       " array([5.1, 3.4, 1.5, 0.2]),\n",
       " array([5. , 3.5, 1.3, 0.3]),\n",
       " array([4.5, 2.3, 1.3, 0.3]),\n",
       " array([4.4, 3.2, 1.3, 0.2]),\n",
       " array([5. , 3.5, 1.6, 0.6]),\n",
       " array([5.1, 3.8, 1.9, 0.4]),\n",
       " array([4.8, 3. , 1.4, 0.3]),\n",
       " array([5.1, 3.8, 1.6, 0.2]),\n",
       " array([4.6, 3.2, 1.4, 0.2]),\n",
       " array([5.3, 3.7, 1.5, 0.2]),\n",
       " array([5. , 3.3, 1.4, 0.2]),\n",
       " array([7. , 3.2, 4.7, 1.4]),\n",
       " array([6.4, 3.2, 4.5, 1.5]),\n",
       " array([6.9, 3.1, 4.9, 1.5]),\n",
       " array([5.5, 2.3, 4. , 1.3]),\n",
       " array([6.5, 2.8, 4.6, 1.5]),\n",
       " array([5.7, 2.8, 4.5, 1.3]),\n",
       " array([6.3, 3.3, 4.7, 1.6]),\n",
       " array([4.9, 2.4, 3.3, 1. ]),\n",
       " array([6.6, 2.9, 4.6, 1.3]),\n",
       " array([5.2, 2.7, 3.9, 1.4]),\n",
       " array([5. , 2. , 3.5, 1. ]),\n",
       " array([5.9, 3. , 4.2, 1.5]),\n",
       " array([6. , 2.2, 4. , 1. ]),\n",
       " array([6.1, 2.9, 4.7, 1.4]),\n",
       " array([5.6, 2.9, 3.6, 1.3]),\n",
       " array([6.7, 3.1, 4.4, 1.4]),\n",
       " array([5.6, 3. , 4.5, 1.5]),\n",
       " array([5.8, 2.7, 4.1, 1. ]),\n",
       " array([6.2, 2.2, 4.5, 1.5]),\n",
       " array([5.6, 2.5, 3.9, 1.1]),\n",
       " array([5.9, 3.2, 4.8, 1.8]),\n",
       " array([6.1, 2.8, 4. , 1.3]),\n",
       " array([6.3, 2.5, 4.9, 1.5]),\n",
       " array([6.1, 2.8, 4.7, 1.2]),\n",
       " array([6.4, 2.9, 4.3, 1.3]),\n",
       " array([6.6, 3. , 4.4, 1.4]),\n",
       " array([6.8, 2.8, 4.8, 1.4]),\n",
       " array([6.7, 3. , 5. , 1.7]),\n",
       " array([6. , 2.9, 4.5, 1.5]),\n",
       " array([5.7, 2.6, 3.5, 1. ]),\n",
       " array([5.5, 2.4, 3.8, 1.1]),\n",
       " array([5.5, 2.4, 3.7, 1. ]),\n",
       " array([5.8, 2.7, 3.9, 1.2]),\n",
       " array([6. , 2.7, 5.1, 1.6]),\n",
       " array([5.4, 3. , 4.5, 1.5]),\n",
       " array([6. , 3.4, 4.5, 1.6]),\n",
       " array([6.7, 3.1, 4.7, 1.5]),\n",
       " array([6.3, 2.3, 4.4, 1.3]),\n",
       " array([5.6, 3. , 4.1, 1.3]),\n",
       " array([5.5, 2.5, 4. , 1.3]),\n",
       " array([5.5, 2.6, 4.4, 1.2]),\n",
       " array([6.1, 3. , 4.6, 1.4]),\n",
       " array([5.8, 2.6, 4. , 1.2]),\n",
       " array([5. , 2.3, 3.3, 1. ]),\n",
       " array([5.6, 2.7, 4.2, 1.3]),\n",
       " array([5.7, 3. , 4.2, 1.2]),\n",
       " array([5.7, 2.9, 4.2, 1.3]),\n",
       " array([6.2, 2.9, 4.3, 1.3]),\n",
       " array([5.1, 2.5, 3. , 1.1]),\n",
       " array([5.7, 2.8, 4.1, 1.3]),\n",
       " array([6.3, 3.3, 6. , 2.5]),\n",
       " array([5.8, 2.7, 5.1, 1.9]),\n",
       " array([7.1, 3. , 5.9, 2.1]),\n",
       " array([6.3, 2.9, 5.6, 1.8]),\n",
       " array([6.5, 3. , 5.8, 2.2]),\n",
       " array([7.6, 3. , 6.6, 2.1]),\n",
       " array([4.9, 2.5, 4.5, 1.7]),\n",
       " array([7.3, 2.9, 6.3, 1.8]),\n",
       " array([6.7, 2.5, 5.8, 1.8]),\n",
       " array([7.2, 3.6, 6.1, 2.5]),\n",
       " array([6.5, 3.2, 5.1, 2. ]),\n",
       " array([6.4, 2.7, 5.3, 1.9]),\n",
       " array([6.8, 3. , 5.5, 2.1]),\n",
       " array([5.7, 2.5, 5. , 2. ]),\n",
       " array([5.8, 2.8, 5.1, 2.4]),\n",
       " array([6.4, 3.2, 5.3, 2.3]),\n",
       " array([6.5, 3. , 5.5, 1.8]),\n",
       " array([7.7, 3.8, 6.7, 2.2]),\n",
       " array([7.7, 2.6, 6.9, 2.3]),\n",
       " array([6. , 2.2, 5. , 1.5]),\n",
       " array([6.9, 3.2, 5.7, 2.3]),\n",
       " array([5.6, 2.8, 4.9, 2. ]),\n",
       " array([7.7, 2.8, 6.7, 2. ]),\n",
       " array([6.3, 2.7, 4.9, 1.8]),\n",
       " array([6.7, 3.3, 5.7, 2.1]),\n",
       " array([7.2, 3.2, 6. , 1.8]),\n",
       " array([6.2, 2.8, 4.8, 1.8]),\n",
       " array([6.1, 3. , 4.9, 1.8]),\n",
       " array([6.4, 2.8, 5.6, 2.1]),\n",
       " array([7.2, 3. , 5.8, 1.6]),\n",
       " array([7.4, 2.8, 6.1, 1.9]),\n",
       " array([7.9, 3.8, 6.4, 2. ]),\n",
       " array([6.4, 2.8, 5.6, 2.2]),\n",
       " array([6.3, 2.8, 5.1, 1.5]),\n",
       " array([6.1, 2.6, 5.6, 1.4]),\n",
       " array([7.7, 3. , 6.1, 2.3]),\n",
       " array([6.3, 3.4, 5.6, 2.4]),\n",
       " array([6.4, 3.1, 5.5, 1.8]),\n",
       " array([6. , 3. , 4.8, 1.8]),\n",
       " array([6.9, 3.1, 5.4, 2.1]),\n",
       " array([6.7, 3.1, 5.6, 2.4]),\n",
       " array([6.9, 3.1, 5.1, 2.3]),\n",
       " array([5.8, 2.7, 5.1, 1.9]),\n",
       " array([6.8, 3.2, 5.9, 2.3]),\n",
       " array([6.7, 3.3, 5.7, 2.5]),\n",
       " array([6.7, 3. , 5.2, 2.3]),\n",
       " array([6.3, 2.5, 5. , 1.9]),\n",
       " array([6.5, 3. , 5.2, 2. ]),\n",
       " array([6.2, 3.4, 5.4, 2.3]),\n",
       " array([5.9, 3. , 5.1, 1.8])]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "list(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ta8YqInTh5v5"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HERt3drbhX0i"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- you can get the features using .data method\n",
    "- you can get the features using .target method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cV-_qHAHyvE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target_names'])\n",
    "data1.head()\n",
    "X= iris.data\n",
    "y = iris.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qg1A2lkUjFak"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### Create train and test data\n",
    "- use train_test_split to get train and test set\n",
    "- set a random_state\n",
    "- test_size: 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYKNJL85h7pQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0KVP17Ozaix"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIjqxbhWv1zv"
   },
   "source": [
    "### One-hot encode the labels\n",
    "- convert class vectors (integers) to binary class matrix\n",
    "- convert labels\n",
    "- number of classes: 3\n",
    "- we are doing this to use categorical_crossentropy as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9vv-_gpyLY9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np_utils.to_categorical( y_train, num_classes = 3)\n",
    "y_test=np_utils.to_categorical( y_test, num_classes = 3)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(112, 3)\n",
      "(38, 4)\n",
      "(38, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovjLyYzWkO9s"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbIFzoPNSyYo"
   },
   "source": [
    "### Initialize a sequential model\n",
    "- Define a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FvSbf1UjHtl"
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGMy999vlacX"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72ibK5Jxm8iL"
   },
   "source": [
    "### Add a layer\n",
    "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
    "- Apply Softmax on Dense Layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZKrBNSRm_o9"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4uiTH8plmNX"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJL8n8vcSyYz"
   },
   "source": [
    "### Compile the model\n",
    "- Use SGD as Optimizer\n",
    "- Use categorical_crossentropy as loss function\n",
    "- Use accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc_-fjIEk1ve"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sihIGbRll_jT"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54ZZCfNGlu0i"
   },
   "source": [
    "### Summarize the model\n",
    "- Check model layers\n",
    "- Understand number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elER3F_4ln8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PiP7j3Vmj4p"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWdbfFCXmCHt"
   },
   "source": [
    "### Fit the model\n",
    "- Give train data as training features and labels\n",
    "- Epochs: 100\n",
    "- Give validation data as testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cO1c-5tjmBVZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112 samples, validate on 38 samples\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 1.0029 - accuracy: 0.6875 - val_loss: 1.0912 - val_accuracy: 0.2368\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 1.0008 - accuracy: 0.3661 - val_loss: 1.1034 - val_accuracy: 0.2368\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9967 - accuracy: 0.5804 - val_loss: 1.0999 - val_accuracy: 0.2368\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9947 - accuracy: 0.4375 - val_loss: 1.0943 - val_accuracy: 0.2368\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 393us/step - loss: 0.9901 - accuracy: 0.4018 - val_loss: 1.0885 - val_accuracy: 0.2368\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9874 - accuracy: 0.3661 - val_loss: 1.0880 - val_accuracy: 0.2368\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9852 - accuracy: 0.3661 - val_loss: 1.0870 - val_accuracy: 0.2368\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 393us/step - loss: 0.9836 - accuracy: 0.5714 - val_loss: 1.0894 - val_accuracy: 0.5789\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9817 - accuracy: 0.6964 - val_loss: 1.0853 - val_accuracy: 0.5789\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9800 - accuracy: 0.6964 - val_loss: 1.0868 - val_accuracy: 0.5789\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9741 - accuracy: 0.6964 - val_loss: 1.0791 - val_accuracy: 0.5789\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9728 - accuracy: 0.5893 - val_loss: 1.0724 - val_accuracy: 0.5789\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9683 - accuracy: 0.5893 - val_loss: 1.0772 - val_accuracy: 0.5789\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 357us/step - loss: 0.9679 - accuracy: 0.6964 - val_loss: 1.0857 - val_accuracy: 0.5789\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9652 - accuracy: 0.6964 - val_loss: 1.0783 - val_accuracy: 0.5789\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9598 - accuracy: 0.6964 - val_loss: 1.0673 - val_accuracy: 0.5789\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9574 - accuracy: 0.6964 - val_loss: 1.0665 - val_accuracy: 0.5789\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9554 - accuracy: 0.6964 - val_loss: 1.0652 - val_accuracy: 0.5789\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9509 - accuracy: 0.6964 - val_loss: 1.0620 - val_accuracy: 0.5789\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9506 - accuracy: 0.6964 - val_loss: 1.0584 - val_accuracy: 0.5789\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9452 - accuracy: 0.6964 - val_loss: 1.0611 - val_accuracy: 0.5789\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9421 - accuracy: 0.6964 - val_loss: 1.0576 - val_accuracy: 0.5789\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9427 - accuracy: 0.6964 - val_loss: 1.0612 - val_accuracy: 0.5789\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 323us/step - loss: 0.9368 - accuracy: 0.6964 - val_loss: 1.0506 - val_accuracy: 0.5789\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9334 - accuracy: 0.6964 - val_loss: 1.0497 - val_accuracy: 0.5789\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9308 - accuracy: 0.6964 - val_loss: 1.0492 - val_accuracy: 0.5789\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9281 - accuracy: 0.6964 - val_loss: 1.0397 - val_accuracy: 0.5789\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9251 - accuracy: 0.6964 - val_loss: 1.0397 - val_accuracy: 0.5789\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9230 - accuracy: 0.6964 - val_loss: 1.0298 - val_accuracy: 0.5789\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9219 - accuracy: 0.6964 - val_loss: 1.0332 - val_accuracy: 0.5789\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9189 - accuracy: 0.6964 - val_loss: 1.0420 - val_accuracy: 0.5789\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9144 - accuracy: 0.6964 - val_loss: 1.0361 - val_accuracy: 0.5789\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9139 - accuracy: 0.6964 - val_loss: 1.0289 - val_accuracy: 0.5789\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.9088 - accuracy: 0.6964 - val_loss: 1.0386 - val_accuracy: 0.5789\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9050 - accuracy: 0.6964 - val_loss: 1.0256 - val_accuracy: 0.5789\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.9036 - accuracy: 0.6964 - val_loss: 1.0153 - val_accuracy: 0.5789\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8987 - accuracy: 0.6964 - val_loss: 1.0148 - val_accuracy: 0.5789\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8964 - accuracy: 0.6964 - val_loss: 1.0131 - val_accuracy: 0.5789\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8923 - accuracy: 0.6964 - val_loss: 1.0137 - val_accuracy: 0.5789\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8903 - accuracy: 0.6964 - val_loss: 1.0121 - val_accuracy: 0.5789\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8864 - accuracy: 0.6964 - val_loss: 1.0116 - val_accuracy: 0.5789\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8860 - accuracy: 0.6964 - val_loss: 1.0066 - val_accuracy: 0.5789\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8810 - accuracy: 0.6964 - val_loss: 1.0000 - val_accuracy: 0.5789\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8769 - accuracy: 0.6964 - val_loss: 0.9963 - val_accuracy: 0.5789\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8742 - accuracy: 0.6964 - val_loss: 0.9948 - val_accuracy: 0.5789\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8732 - accuracy: 0.6964 - val_loss: 0.9873 - val_accuracy: 0.5789\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8683 - accuracy: 0.6964 - val_loss: 0.9887 - val_accuracy: 0.5789\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8673 - accuracy: 0.6964 - val_loss: 0.9787 - val_accuracy: 0.5789\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8618 - accuracy: 0.6964 - val_loss: 0.9811 - val_accuracy: 0.5789\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 287us/step - loss: 0.8589 - accuracy: 0.6964 - val_loss: 0.9771 - val_accuracy: 0.5789\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8552 - accuracy: 0.6964 - val_loss: 0.9746 - val_accuracy: 0.5789\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8529 - accuracy: 0.6964 - val_loss: 0.9700 - val_accuracy: 0.5789\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8491 - accuracy: 0.6964 - val_loss: 0.9646 - val_accuracy: 0.5789\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8475 - accuracy: 0.6964 - val_loss: 0.9565 - val_accuracy: 0.6053\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8431 - accuracy: 0.6964 - val_loss: 0.9552 - val_accuracy: 0.5789\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8404 - accuracy: 0.6964 - val_loss: 0.9578 - val_accuracy: 0.5789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8379 - accuracy: 0.6964 - val_loss: 0.9521 - val_accuracy: 0.5789\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8348 - accuracy: 0.6964 - val_loss: 0.9549 - val_accuracy: 0.5789\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8317 - accuracy: 0.6964 - val_loss: 0.9460 - val_accuracy: 0.5789\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8275 - accuracy: 0.6964 - val_loss: 0.9402 - val_accuracy: 0.5789\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8245 - accuracy: 0.6964 - val_loss: 0.9382 - val_accuracy: 0.5789\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8220 - accuracy: 0.6964 - val_loss: 0.9336 - val_accuracy: 0.5789\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8214 - accuracy: 0.6964 - val_loss: 0.9300 - val_accuracy: 0.5789\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8158 - accuracy: 0.6964 - val_loss: 0.9277 - val_accuracy: 0.5789\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8134 - accuracy: 0.6964 - val_loss: 0.9240 - val_accuracy: 0.5789\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8095 - accuracy: 0.6964 - val_loss: 0.9224 - val_accuracy: 0.5789\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.8061 - accuracy: 0.6964 - val_loss: 0.9183 - val_accuracy: 0.5789\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8040 - accuracy: 0.6964 - val_loss: 0.9083 - val_accuracy: 0.6053\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.8001 - accuracy: 0.6964 - val_loss: 0.9055 - val_accuracy: 0.6053\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7981 - accuracy: 0.6964 - val_loss: 0.9045 - val_accuracy: 0.6053\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7939 - accuracy: 0.6964 - val_loss: 0.9036 - val_accuracy: 0.5789\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7910 - accuracy: 0.6964 - val_loss: 0.8988 - val_accuracy: 0.6053\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7878 - accuracy: 0.6964 - val_loss: 0.8971 - val_accuracy: 0.5789\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7854 - accuracy: 0.6964 - val_loss: 0.8949 - val_accuracy: 0.5789\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7820 - accuracy: 0.6964 - val_loss: 0.8902 - val_accuracy: 0.5789\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7786 - accuracy: 0.6964 - val_loss: 0.8850 - val_accuracy: 0.6053\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 287us/step - loss: 0.7756 - accuracy: 0.6964 - val_loss: 0.8835 - val_accuracy: 0.5789\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7726 - accuracy: 0.6964 - val_loss: 0.8788 - val_accuracy: 0.6053\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7696 - accuracy: 0.6964 - val_loss: 0.8721 - val_accuracy: 0.6053\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7669 - accuracy: 0.6964 - val_loss: 0.8662 - val_accuracy: 0.6053\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7639 - accuracy: 0.6964 - val_loss: 0.8633 - val_accuracy: 0.6053\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7605 - accuracy: 0.6964 - val_loss: 0.8625 - val_accuracy: 0.6053\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7582 - accuracy: 0.6964 - val_loss: 0.8575 - val_accuracy: 0.6053\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7554 - accuracy: 0.6964 - val_loss: 0.8545 - val_accuracy: 0.6053\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7518 - accuracy: 0.6964 - val_loss: 0.8504 - val_accuracy: 0.6053\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7494 - accuracy: 0.6964 - val_loss: 0.8477 - val_accuracy: 0.6053\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 268us/step - loss: 0.7465 - accuracy: 0.6964 - val_loss: 0.8484 - val_accuracy: 0.6053\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7441 - accuracy: 0.6964 - val_loss: 0.8411 - val_accuracy: 0.6053\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7401 - accuracy: 0.6964 - val_loss: 0.8379 - val_accuracy: 0.6053\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7376 - accuracy: 0.6964 - val_loss: 0.8335 - val_accuracy: 0.6053\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 296us/step - loss: 0.7352 - accuracy: 0.6964 - val_loss: 0.8258 - val_accuracy: 0.6053\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7323 - accuracy: 0.6964 - val_loss: 0.8249 - val_accuracy: 0.6053\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7288 - accuracy: 0.6964 - val_loss: 0.8213 - val_accuracy: 0.6053\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 285us/step - loss: 0.7261 - accuracy: 0.6964 - val_loss: 0.8166 - val_accuracy: 0.6053\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7242 - accuracy: 0.6964 - val_loss: 0.8101 - val_accuracy: 0.6053\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7222 - accuracy: 0.6964 - val_loss: 0.8087 - val_accuracy: 0.6053\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 286us/step - loss: 0.7180 - accuracy: 0.6964 - val_loss: 0.8089 - val_accuracy: 0.6053\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7159 - accuracy: 0.6964 - val_loss: 0.8056 - val_accuracy: 0.6053\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7127 - accuracy: 0.6964 - val_loss: 0.8032 - val_accuracy: 0.6053\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 321us/step - loss: 0.7098 - accuracy: 0.6964 - val_loss: 0.7979 - val_accuracy: 0.6053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2589101ff88>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,nb_epoch=100,validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re9ItAR3yS3J"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liw0IFf9yVqH"
   },
   "source": [
    "### Make predictions\n",
    "- Predict labels on one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5sBybi6mlLl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_test = X_test[10]\n",
    "single_test = single_test.reshape(1,4)\n",
    "model.predict_classes(single_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSUgMq3m0bG7"
   },
   "source": [
    "### Compare the prediction with actual label\n",
    "- Print the same row as done in the previous step but of actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5WbwVPyz-qQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.6 2.7 4.2 1.3]\n",
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[10])\n",
    "print(y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrTKwbgE7NFT"
   },
   "source": [
    "\n",
    "\n",
    "Conclusion:\n",
    "Model has predicted value accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1UBYPNp5Tn1"
   },
   "source": [
    "# Stock prices dataset\n",
    "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
    "\n",
    "## Description\n",
    "A brief description of columns.\n",
    "- open: The opening market price of the equity symbol on the date\n",
    "- high: The highest market price of the equity symbol on the date\n",
    "- low: The lowest recorded market price of the equity symbol on the date\n",
    "- close: The closing recorded price of the equity symbol on the date\n",
    "- symbol: Symbol of the listed company\n",
    "- volume: Total traded volume of the equity symbol on the date\n",
    "- date: Date of record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctH_ZW5g-M3g"
   },
   "source": [
    "### Specifying the TensorFlow version\n",
    "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQbdODpH-M3r"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFQWH1tj-M38"
   },
   "source": [
    "### Import TensorFlow\n",
    "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho5n-xhd-M3_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgkl0qu6-M4F"
   },
   "source": [
    "### Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKgTyuA3-M4G"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_88voqAH-O6J"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRHCeJqP-evf"
   },
   "source": [
    "### Load the data\n",
    "- load the csv file and read it using pandas\n",
    "- file name is prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKVH5v7r-RmC"
   },
   "outputs": [],
   "source": [
    "# run this cell to upload file if you are using google colab\n",
    "#from google.colab import files\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gDC6cSW_FSK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prices.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851264, 7)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlLKVPVH_BCT"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J4BlzVA_gZd"
   },
   "source": [
    "### Drop columnns\n",
    "- drop \"date\" and \"symbol\" column from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKEK8aEE_Csx"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['date','symbol'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>115.510002</td>\n",
       "      <td>115.550003</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>116.059998</td>\n",
       "      <td>1098000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116.459999</td>\n",
       "      <td>112.849998</td>\n",
       "      <td>112.589996</td>\n",
       "      <td>117.070000</td>\n",
       "      <td>949600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113.510002</td>\n",
       "      <td>114.379997</td>\n",
       "      <td>110.050003</td>\n",
       "      <td>115.029999</td>\n",
       "      <td>785300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113.330002</td>\n",
       "      <td>112.529999</td>\n",
       "      <td>111.919998</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>1093700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113.660004</td>\n",
       "      <td>110.379997</td>\n",
       "      <td>109.870003</td>\n",
       "      <td>115.870003</td>\n",
       "      <td>1523500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0\n",
       "5  115.510002  115.550003  114.500000  116.059998  1098000.0\n",
       "6  116.459999  112.849998  112.589996  117.070000   949600.0\n",
       "7  113.510002  114.379997  110.050003  115.029999   785300.0\n",
       "8  113.330002  112.529999  111.919998  114.879997  1093700.0\n",
       "9  113.660004  110.379997  109.870003  115.870003  1523500.0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTPhO6v-AiZt"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsZXmF3NAkna"
   },
   "source": [
    "### Take initial rows\n",
    "- Take first 1000 rows from the data\n",
    "- This step is done to make the execution faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKs04iIHAjxN"
   },
   "outputs": [],
   "source": [
    "data_sample = data.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vGtnapgBIJm"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8u_jlbABTip"
   },
   "source": [
    "### Get features and label from the dataset in separate variable\n",
    "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
    "- Take \"volume\" column as label\n",
    "- Normalize label column by dividing it with 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQjCMzUXBJbg"
   },
   "outputs": [],
   "source": [
    "data_sample['volume'] = data_sample['volume'] / 1000000\n",
    "\n",
    "y = data_sample['volume']\n",
    "X = data_sample.drop('volume',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.1636\n",
       "1    2.3864\n",
       "2    2.4895\n",
       "3    2.0063\n",
       "4    1.4086\n",
       "5    1.0980\n",
       "6    0.9496\n",
       "7    0.7853\n",
       "8    1.0937\n",
       "9    1.5235\n",
       "Name: volume, dtype: float64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTAKzlxZBz0z"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfY8Km1Zzyt2"
   },
   "source": [
    "### Convert data\n",
    "- Convert features and labels to numpy array\n",
    "- Convert their data type to \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ko7nnQVbYENh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_arr=np.array(X).astype('float32')\n",
    "y_arr=np.array(y).astype('float32')\n",
    "print(type(X_arr))\n",
    "print(type(y_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TWpN0nVTpUx"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ1FKEs-4btX"
   },
   "source": [
    "### Normalize data\n",
    "- Normalize features\n",
    "- Use tf.math.l2_normalize to normalize features\n",
    "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0Tfe00X78wB"
   },
   "outputs": [],
   "source": [
    "X_arr=tf.nn.l2_normalize(X_arr, axis=None, epsilon=1e-12, name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02202893 0.02245905 0.02182904 0.02253223]\n",
      " [0.02235197 0.0214132  0.02140606 0.02240551]\n",
      " [0.0207707  0.02051548 0.02051191 0.02137037]\n",
      " ...\n",
      " [0.00505436 0.00513467 0.00499903 0.00514181]\n",
      " [0.00785282 0.00799559 0.0078082  0.00799738]\n",
      " [0.00643931 0.00662849 0.00642682 0.00664455]], shape=(1000, 4), dtype=float32)\n",
      "[2.163600e+00 2.386400e+00 2.489500e+00 2.006300e+00 1.408600e+00\n",
      " 1.098000e+00 9.496000e-01 7.853000e-01 1.093700e+00 1.523500e+00\n",
      " 1.653900e+00 9.443000e-01 7.449000e-01 7.038000e-01 5.631000e-01\n",
      " 8.961000e-01 6.804000e-01 7.499000e-01 5.742000e-01 6.948000e-01\n",
      " 8.963000e-01 9.563000e-01 9.971000e-01 1.200500e+00 1.725200e+00\n",
      " 1.946000e+00 1.319500e+00 9.224000e-01 1.185100e+00 9.215000e-01\n",
      " 4.409000e-01 1.244300e+00 6.813000e-01 4.112000e-01 4.473000e-01\n",
      " 5.592000e-01 4.599000e-01 9.716000e-01 6.941000e-01 1.159600e+00\n",
      " 8.847000e-01 6.920000e-01 8.980000e-01 8.445000e-01 7.965000e-01\n",
      " 4.168000e-01 5.902000e-01 4.951000e-01 5.158000e-01 5.621000e-01\n",
      " 8.093000e-01 1.607300e+00 1.001300e+00 7.114000e-01 4.442000e-01\n",
      " 5.341000e-01 7.982000e-01 5.910000e-01 4.432000e-01 5.780000e-01\n",
      " 7.651000e-01 5.143000e-01 1.150000e+00 2.035300e+00 7.022000e-01\n",
      " 6.891000e-01 5.532000e-01 7.759000e-01 6.217000e-01 3.581000e-01\n",
      " 9.393000e-01 7.355000e-01 9.246000e-01 6.588000e-01 1.055400e+00\n",
      " 6.953000e-01 4.600000e-01 5.329000e-01 4.626000e-01 9.107000e-01\n",
      " 8.896000e-01 7.347000e-01 1.304800e+00 1.597300e+00 1.917500e+00\n",
      " 3.221500e+00 9.198000e-01 1.991600e+00 5.948000e-01 4.758000e-01\n",
      " 7.151000e-01 5.285000e-01 5.800000e-01 6.297000e-01 5.982000e-01\n",
      " 8.167000e-01 6.408000e-01 7.611000e-01 4.621000e-01 6.270000e-01\n",
      " 7.225000e-01 1.384200e+00 4.415000e-01 4.236000e-01 4.967000e-01\n",
      " 5.367000e-01 4.294000e-01 7.191000e-01 3.945000e-01 5.480000e-01\n",
      " 2.633000e-01 3.293000e-01 3.787000e-01 4.048000e-01 6.593000e-01\n",
      " 5.503000e-01 4.856000e-01 5.619000e-01 5.123000e-01 1.638000e+00\n",
      " 8.844000e-01 7.952000e-01 1.049700e+00 1.162100e+00 8.584000e-01\n",
      " 7.614000e-01 7.499000e-01 5.911000e-01 6.928000e-01 4.826000e-01\n",
      " 8.440000e-01 6.300000e-01 5.671000e-01 5.122000e-01 4.622000e-01\n",
      " 7.061000e-01 5.339000e-01 3.747000e-01 3.738000e-01 3.657000e-01\n",
      " 3.139000e-01 4.010000e-01 2.755000e-01 4.267000e-01 5.746000e-01\n",
      " 7.932000e-01 4.469000e-01 7.667000e-01 1.865200e+00 9.308000e-01\n",
      " 5.027000e-01 7.097000e-01 5.639000e-01 3.429000e-01 4.788000e-01\n",
      " 5.018000e-01 4.103000e-01 4.438000e-01 1.414800e+00 5.707000e-01\n",
      " 3.913000e-01 4.252000e-01 5.263000e-01 5.147000e-01 4.565000e-01\n",
      " 4.141000e-01 4.645000e-01 5.262000e-01 4.873000e-01 7.523000e-01\n",
      " 6.325000e-01 7.283000e-01 6.411000e-01 6.555000e-01 4.889000e-01\n",
      " 7.419000e-01 7.134000e-01 1.847200e+00 3.107000e-01 3.110000e-01\n",
      " 9.277000e-01 7.165000e-01 7.970000e-01 4.103000e-01 5.036000e-01\n",
      " 5.672000e-01 1.504500e+00 1.461900e+00 1.142100e+00 1.068500e+00\n",
      " 7.002000e-01 7.672000e-01 8.551000e-01 4.238000e-01 5.757000e-01\n",
      " 6.819000e-01 5.403000e-01 5.480000e-01 5.645000e-01 5.056000e-01\n",
      " 4.356000e-01 5.548000e-01 4.966000e-01 3.120000e-01 4.147000e-01\n",
      " 4.783000e-01 4.170000e-01 4.457000e-01 4.592000e-01 4.758000e-01\n",
      " 6.932000e-01 7.367000e-01 3.531200e+00 1.345600e+00 9.676000e-01\n",
      " 1.291600e+00 1.524500e+00 7.543000e-01 1.089100e+00 6.397000e-01\n",
      " 8.184000e-01 5.245000e-01 8.190000e-01 4.759000e-01 1.011700e+00\n",
      " 3.802000e-01 2.840000e-01 4.399000e-01 3.529000e-01 6.767000e-01\n",
      " 8.987000e-01 8.558000e-01 6.047000e-01 1.301200e+00 9.283000e-01\n",
      " 9.885000e-01 9.434000e-01 8.225000e-01 1.168200e+00 8.464000e-01\n",
      " 8.266000e-01 1.232700e+00 9.376000e-01 6.515000e-01 6.146000e-01\n",
      " 5.572000e-01 3.619000e-01 3.829000e-01 4.299000e-01 2.166000e-01\n",
      " 4.664000e-01 3.815500e+00 9.837300e+00 1.701700e+00 1.234324e+02\n",
      " 2.455900e+00 1.082900e+01 3.650100e+00 4.710200e+00 2.102700e+00\n",
      " 3.472500e+00 3.930100e+00 7.943000e-01 2.228600e+00 1.299300e+00\n",
      " 4.076600e+00 4.597600e+00 5.671300e+00 2.362000e+00 8.564000e-01\n",
      " 7.750900e+00 1.228200e+00 3.793000e-01 3.015600e+00 7.129000e-01\n",
      " 1.802400e+00 2.631000e+00 1.128200e+00 1.861510e+01 8.767000e-01\n",
      " 3.061000e-01 5.277400e+00 2.238700e+00 2.750500e+00 7.599900e+00\n",
      " 1.650400e+00 4.641800e+00 3.407100e+00 2.364900e+00 3.324500e+00\n",
      " 1.131400e+00 2.457200e+00 1.151210e+01 9.306600e+00 1.483400e+00\n",
      " 5.387000e-01 1.301200e+00 2.176100e+00 6.894300e+00 8.292000e-01\n",
      " 4.083000e-01 6.186700e+00 1.808452e+02 1.146750e+01 2.971500e+00\n",
      " 4.550800e+00 6.433800e+00 1.371800e+00 1.793200e+00 3.746700e+00\n",
      " 5.888000e+00 2.469700e+00 6.127700e+00 2.387000e-01 1.234200e+00\n",
      " 1.437610e+01 1.433230e+01 1.286000e+00 1.511500e+00 4.067930e+01\n",
      " 3.385100e+00 4.009700e+00 3.824400e+00 7.325600e+00 2.670300e+00\n",
      " 4.553000e+00 6.710900e+00 1.964100e+00 5.372700e+00 4.832000e+00\n",
      " 3.055200e+00 6.172000e+00 2.232400e+00 3.114680e+01 8.229000e-01\n",
      " 3.227300e+00 1.261400e+00 4.874200e+00 1.014300e+00 1.438400e+00\n",
      " 1.357340e+01 2.660500e+00 3.139000e-01 1.148600e+00 5.875500e+00\n",
      " 6.600000e-01 2.239700e+00 4.447400e+00 4.514800e+00 3.350600e+00\n",
      " 8.281000e-01 4.812000e-01 1.388080e+01 3.280200e+00 1.574200e+00\n",
      " 7.906000e+00 5.985370e+01 8.391000e+00 1.051100e+00 2.068100e+00\n",
      " 5.765200e+00 1.680700e+00 1.467680e+01 1.017380e+01 3.025000e-01\n",
      " 2.175500e+00 1.448250e+01 6.017600e+00 3.974600e+00 7.552600e+00\n",
      " 4.372000e-01 1.046000e+00 5.799100e+00 5.202600e+00 1.370040e+01\n",
      " 2.844100e+00 5.354000e-01 1.306900e+00 3.059400e+00 4.995000e-01\n",
      " 1.502200e+00 1.693300e+01 1.199700e+00 2.228100e+00 1.421600e+00\n",
      " 4.152100e+00 1.910400e+00 4.225500e+00 3.850500e+00 2.251160e+01\n",
      " 1.009500e+00 2.142300e+00 7.507000e-01 1.931500e+00 3.508400e+00\n",
      " 1.446400e+00 3.781000e+00 2.401200e+00 4.110000e+00 5.763000e-01\n",
      " 3.299500e+00 7.742000e-01 1.206800e+00 3.687800e+00 4.480000e-01\n",
      " 5.231900e+00 2.347600e+00 1.049400e+00 1.030800e+00 4.367600e+00\n",
      " 1.062400e+00 2.186500e+00 7.744000e-01 6.085580e+01 3.432000e+00\n",
      " 1.808240e+01 3.215100e+00 1.840900e+00 6.749000e-01 4.623700e+00\n",
      " 2.536000e+00 1.542910e+01 1.873800e+00 2.269800e+00 3.553300e+00\n",
      " 2.161800e+00 1.789400e+00 2.849400e+00 1.651270e+01 6.624000e-01\n",
      " 1.636000e+00 1.908400e+00 4.030400e+00 1.198500e+00 6.707990e+01\n",
      " 2.997000e+00 1.680900e+01 5.156400e+00 1.653910e+01 3.927000e+00\n",
      " 3.908400e+00 8.369000e-01 2.533400e+00 9.185600e+00 2.098800e+00\n",
      " 9.135000e+00 3.905600e+00 5.352000e-01 1.157160e+01 6.076000e-01\n",
      " 1.652600e+00 1.046280e+01 3.658400e+00 1.426600e+00 4.278700e+00\n",
      " 1.312090e+01 3.729300e+00 6.944100e+00 2.904000e+00 4.011600e+00\n",
      " 7.389200e+00 9.200000e-01 2.795740e+01 3.018000e+00 2.705200e+00\n",
      " 6.546000e-01 7.948000e-01 6.997800e+00 1.048000e+00 2.008500e+00\n",
      " 6.155300e+00 3.637000e+00 3.252000e-01 2.860000e-01 1.793700e+00\n",
      " 4.780090e+01 2.353000e+00 4.034900e+00 4.444300e+00 2.965200e+00\n",
      " 9.955000e-01 3.371000e-01 2.728100e+00 4.254200e+00 1.876000e+00\n",
      " 8.727000e+00 1.728500e+00 9.506200e+00 3.332500e+00 3.546050e+01\n",
      " 2.816600e+00 2.749000e+00 1.490160e+01 6.263800e+00 2.827000e+00\n",
      " 1.634900e+00 2.122800e+00 1.387040e+01 1.271880e+01 3.239900e+00\n",
      " 6.135000e-01 2.210900e+00 3.959400e+00 1.241400e+00 4.493800e+00\n",
      " 8.834000e-01 1.656200e+00 8.989000e-01 2.887600e+00 6.067100e+00\n",
      " 2.408300e+00 3.811400e+00 1.332800e+00 9.616700e+00 1.856900e+00\n",
      " 9.378000e-01 9.625000e+00 1.141850e+01 8.823100e+00 9.321000e+00\n",
      " 3.858000e-01 2.642400e+00 2.860300e+00 3.380500e+00 2.975900e+00\n",
      " 5.839300e+00 1.518700e+00 2.035200e+00 1.859700e+00 7.953300e+00\n",
      " 6.109400e+00 4.767000e+00 4.970000e-01 6.010500e+00 4.900000e-01\n",
      " 4.562000e-01 3.680300e+00 3.043700e+00 3.880200e+00 1.104860e+01\n",
      " 3.424300e+00 4.072100e+00 1.389650e+01 9.214200e+00 3.840910e+01\n",
      " 7.511700e+00 7.210000e-01 9.510000e-02 3.441270e+01 1.642300e+00\n",
      " 3.611900e+00 2.227600e+00 2.171500e+00 2.602200e+00 5.625400e+00\n",
      " 1.723960e+01 1.164900e+00 6.905600e+00 1.197240e+01 1.616100e+00\n",
      " 5.385900e+00 1.683700e+00 1.884500e+00 5.320100e+00 1.967200e+00\n",
      " 4.923600e+00 2.000510e+01 2.925800e+00 8.769000e-01 1.759600e+00\n",
      " 1.670800e+00 2.679500e+01 1.278200e+00 3.624100e+00 3.731300e+00\n",
      " 2.932200e+00 2.040000e+00 2.631700e+00 2.041800e+00 8.632000e-01\n",
      " 1.519900e+00 5.130400e+00 6.585900e+00 5.208600e+01 3.470900e+00\n",
      " 9.190800e+00 4.619400e+00 9.610000e-01 6.121300e+00 1.880200e+00\n",
      " 1.169700e+00 7.844500e+00 4.890300e+00 7.125000e-01 8.735000e-01\n",
      " 2.413400e+00 1.126800e+00 5.465000e-01 3.001500e+00 1.579100e+00\n",
      " 5.405000e-01 2.460200e+00 1.233000e+00 1.524100e+00 1.457020e+01\n",
      " 6.407000e-01 3.252000e+00 2.792600e+00 5.103000e-01 1.175480e+01\n",
      " 1.018900e+00 2.060800e+00 3.964700e+00 8.997000e-01 9.040000e-01\n",
      " 9.279000e-01 1.574360e+01 2.244000e+00 1.425500e+00 1.897000e+00\n",
      " 1.637000e+01 8.668000e-01 1.402360e+01 3.100900e+00 9.204000e-01\n",
      " 1.338000e+00 2.947000e-01 7.737000e-01 5.771300e+00 2.415600e+00\n",
      " 2.680000e-01 1.323500e+00 5.119300e+00 3.450400e+00 1.572200e+00\n",
      " 4.008800e+00 4.659000e-01 2.081200e+00 6.668600e+00 3.440300e+00\n",
      " 1.111690e+01 1.418700e+00 2.033000e+00 4.911500e+00 3.473200e+00\n",
      " 3.982800e+00 8.322300e+00 2.459700e+00 2.913660e+01 1.302400e+00\n",
      " 2.928700e+00 1.793700e+00 8.490500e+00 4.589100e+00 1.935200e+00\n",
      " 1.257500e+01 1.734900e+00 7.182800e+00 2.330200e+00 3.716000e+00\n",
      " 1.316400e+00 3.355000e+00 8.833800e+00 1.795200e+00 7.026100e+00\n",
      " 1.036930e+01 3.630600e+00 4.284000e+00 8.785900e+00 1.835200e+00\n",
      " 1.553600e+00 5.947000e-01 1.219950e+01 3.110300e+00 5.894200e+00\n",
      " 3.897200e+00 3.433800e+00 1.692500e+00 1.289170e+01 6.104100e+00\n",
      " 2.018000e+01 7.806000e-01 2.042000e+00 3.132800e+00 1.545450e+01\n",
      " 1.128000e+00 1.863800e+00 3.900000e-01 2.652000e+00 1.744900e+00\n",
      " 1.196900e+00 1.617660e+01 9.544000e-01 8.171000e+00 3.118200e+00\n",
      " 1.541800e+00 3.933570e+01 5.275000e+00 1.010100e+00 2.058800e+00\n",
      " 7.020200e+00 2.075310e+01 4.277900e+00 1.832400e+00 2.071000e+00\n",
      " 4.741400e+00 6.275000e-01 2.670400e+00 2.555900e+00 2.824700e+00\n",
      " 2.780910e+01 1.051400e+00 1.347270e+01 1.658740e+01 2.962300e+00\n",
      " 7.824000e-01 3.974600e+00 1.938700e+00 4.186000e+00 2.521200e+01\n",
      " 1.932400e+00 1.504762e+02 2.476800e+00 1.056210e+01 2.613000e+00\n",
      " 7.108800e+00 2.040100e+00 3.458700e+00 3.252400e+00 3.693000e-01\n",
      " 3.008800e+00 1.422200e+00 5.112400e+00 5.093200e+00 4.573600e+00\n",
      " 3.965300e+00 5.834000e-01 8.920500e+00 1.377600e+00 5.457000e-01\n",
      " 5.421900e+00 3.989000e-01 3.052800e+00 5.342100e+00 9.276000e-01\n",
      " 1.517320e+01 7.141000e-01 3.246000e-01 7.882800e+00 2.782200e+00\n",
      " 2.575800e+00 8.851900e+00 2.532200e+00 8.385500e+00 4.986100e+00\n",
      " 2.464500e+00 4.087500e+00 7.901000e-01 2.705400e+00 2.220100e+01\n",
      " 1.136610e+01 1.341000e+00 3.011000e-01 2.708800e+00 1.843600e+00\n",
      " 1.064120e+01 7.000000e-01 1.224700e+00 8.867800e+00 2.095213e+02\n",
      " 5.993700e+00 4.406600e+00 7.023400e+00 6.979200e+00 1.093600e+00\n",
      " 1.122200e+00 3.276600e+00 1.066340e+01 4.899400e+00 7.189300e+00\n",
      " 1.988000e-01 1.526800e+00 1.697360e+01 8.594200e+00 2.654000e+00\n",
      " 2.173700e+00 6.686170e+01 3.437400e+00 5.517100e+00 3.023700e+00\n",
      " 5.697200e+00 9.280400e+00 4.510400e+00 5.441000e+00 2.116000e+00\n",
      " 3.199900e+00 6.705800e+00 7.324400e+00 6.662500e+00 1.936000e+00\n",
      " 2.869260e+01 2.352700e+00 1.000000e-02 4.564300e+00 1.412800e+00\n",
      " 5.296800e+00 3.386900e+00 1.336700e+00 1.774650e+01 2.835000e+00\n",
      " 5.110000e-01 1.592300e+00 4.288600e+00 8.488000e-01 4.227100e+00\n",
      " 8.029400e+00 6.313200e+00 3.223400e+00 5.152000e-01 3.109000e-01\n",
      " 1.008450e+01 2.775800e+00 2.497100e+00 7.942400e+00 4.512450e+01\n",
      " 1.197120e+01 1.455600e+00 3.450100e+00 8.993400e+00 2.149700e+00\n",
      " 7.512000e+00 1.059370e+01 1.200700e+00 2.802200e+00 2.506600e+01\n",
      " 9.206100e+00 3.007400e+00 7.766400e+00 2.836000e-01 1.535200e+00\n",
      " 1.025450e+01 4.502200e+00 1.030770e+01 3.193000e+00 8.574000e-01\n",
      " 8.415000e-01 3.456600e+00 4.135000e-01 1.027600e+00 1.878380e+01\n",
      " 9.571000e-01 2.735200e+00 2.127200e+00 3.872300e+00 1.541000e+00\n",
      " 3.711400e+00 6.632500e+00 2.668310e+01 1.098400e+00 2.856000e+00\n",
      " 9.488000e-01 2.485100e+00 3.246000e+00 2.269800e+00 2.707500e+00\n",
      " 2.344800e+00 4.450000e+00 6.819000e-01 4.545400e+00 7.623000e-01\n",
      " 2.014200e+00 4.364200e+00 5.423000e-01 5.734800e+00 2.946600e+00\n",
      " 1.157100e+00 9.292000e-01 5.222400e+00 1.393500e+00 2.431800e+00\n",
      " 6.426000e-01 2.156202e+02 2.898800e+00 1.731300e+01 2.493300e+00\n",
      " 2.922100e+00 2.143400e+00 4.868800e+00 2.674000e+00 1.270070e+01\n",
      " 2.914300e+00 1.414100e+00 2.981200e+00 2.363100e+00 1.064000e+00\n",
      " 2.672000e+00 2.219590e+01 7.788000e-01 2.520900e+00 2.206600e+00\n",
      " 3.272900e+00 1.535400e+00 6.455060e+01 2.315000e+00 2.078820e+01\n",
      " 7.439200e+00 1.806090e+01 6.031900e+00 6.003300e+00 9.428000e-01\n",
      " 1.288600e+00 1.829710e+01 1.719300e+00 1.165940e+01 6.883000e+00\n",
      " 5.945000e-01 1.898970e+01 8.226000e-01 1.646900e+00 2.122660e+01\n",
      " 2.300400e+00 1.173300e+00 4.180600e+00 1.559430e+01 3.195800e+00\n",
      " 1.242400e+01 2.805400e+00 2.211800e+00 6.479200e+00 3.039400e+00\n",
      " 2.883050e+01 8.338900e+00 1.854400e+00 8.921000e-01 5.095000e-01\n",
      " 6.070600e+00 3.828900e+00 1.860700e+00 6.841400e+00 7.804000e+00\n",
      " 2.236000e-01 3.489000e-01 3.039700e+00 5.235770e+01 3.142100e+00\n",
      " 5.249500e+00 6.134700e+00 2.522700e+00 1.120200e+00 4.101000e-01\n",
      " 2.247700e+00 2.711400e+00 2.186900e+00 4.925700e+00 1.510300e+00\n",
      " 1.067310e+01 9.555900e+00 4.120830e+01 5.012300e+00 1.472000e+00\n",
      " 1.666080e+01 5.996200e+00 2.136200e+00 2.465900e+00 1.689300e+00\n",
      " 2.317240e+01 1.990400e+01 2.539600e+00 1.129300e+00 1.975600e+00\n",
      " 9.097500e+00 2.242100e+00 6.205100e+00 8.646000e-01 7.092000e-01\n",
      " 1.129600e+00 3.449300e+00 7.517100e+00 2.356500e+00 4.839200e+00\n",
      " 3.684600e+00 1.442840e+01 1.463300e+00 7.496000e-01 1.335510e+01\n",
      " 1.153830e+01 9.931300e+00 1.290600e+01 3.699000e-01 2.701000e+00\n",
      " 2.627800e+00 3.258700e+00 5.206100e+00 7.099000e+00 7.795500e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.133200e+00 1.982400e+00 3.715280e+01 6.568600e+00 5.604300e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X_arr)\n",
    "print(y_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmXUGc2oTspa"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJelDMpzxs0L"
   },
   "source": [
    "### Define weight and bias\n",
    "- Initialize weight and bias with tf.zeros\n",
    "- tf.zeros is an initializer that generates tensors initialized to 0\n",
    "- Specify the value for shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o9RPWVTxs0O"
   },
   "outputs": [],
   "source": [
    "W = tf.zeros(shape=[4,1])\n",
    "b = tf.zeros(shape=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a0wr94aTyjg"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMXXYdOSxs0Q"
   },
   "source": [
    "### Get prediction\n",
    "- Define a function to get prediction\n",
    "- Approach: prediction = (X * W) + b; here is X is features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Cty1y0xs0S"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def prediction(x, w, b):\n",
    "    xw_matmul = tf.matmul(x, w)\n",
    "    y = tf.add(xw_matmul, b)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQmS3Tauxs0V"
   },
   "source": [
    "### Calculate loss\n",
    "- Calculate loss using predictions\n",
    "- Define a function to calculate loss\n",
    "- We are calculating mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FRXmDd5xs0X"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss(y_actual, y_predicted):\n",
    "    diff = y_actual - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbBpnOtfT0wd"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkOzAUUsTmF_"
   },
   "source": [
    "### Define a function to train the model\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R4uieGYLYtM"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(x, y_actual, w, b, learning_rate=0.01):    \n",
    "    #Record math ops on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch([w,b])    \n",
    "        current_prediction = prediction(x, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    #Calculate Gradients for Loss w.r.t Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW4SEP8kT2ls"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeN0deOvT81N"
   },
   "source": [
    "### Train the model for 100 epochs \n",
    "- Observe the training loss at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjkn4gUgLevE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= tf.Tensor(\n",
      "[[0.00126439]\n",
      " [0.00126667]\n",
      " [0.00125157]\n",
      " [0.00127704]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.10628445], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 0 236.16774\n",
      "W= tf.Tensor(\n",
      "[[0.00250346]\n",
      " [0.00250799]\n",
      " [0.00247808]\n",
      " [0.00252851]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.210442], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 1 235.09312\n",
      "W= tf.Tensor(\n",
      "[[0.00371773]\n",
      " [0.00372445]\n",
      " [0.00368004]\n",
      " [0.00375493]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.31251523], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 2 234.06105\n",
      "W= tf.Tensor(\n",
      "[[0.00490768]\n",
      " [0.00491655]\n",
      " [0.00485793]\n",
      " [0.0049568 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.41254583], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 3 233.06989\n",
      "W= tf.Tensor(\n",
      "[[0.00607382]\n",
      " [0.0060848 ]\n",
      " [0.00601224]\n",
      " [0.0061346 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.5105747], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 4 232.11801\n",
      "W= tf.Tensor(\n",
      "[[0.0072166 ]\n",
      " [0.00722965]\n",
      " [0.00714344]\n",
      " [0.00728882]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.6066418], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 5 231.20384\n",
      "W= tf.Tensor(\n",
      "[[0.00833651]\n",
      " [0.00835158]\n",
      " [0.00825199]\n",
      " [0.00841994]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.7007866], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 6 230.32588\n",
      "W= tf.Tensor(\n",
      "[[0.009434  ]\n",
      " [0.00945105]\n",
      " [0.00933835]\n",
      " [0.0095284 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.79304737], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 7 229.48273\n",
      "W= tf.Tensor(\n",
      "[[0.01050951]\n",
      " [0.01052851]\n",
      " [0.01040296]\n",
      " [0.01061468]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.8834619], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 8 228.67294\n",
      "W= tf.Tensor(\n",
      "[[0.01156349]\n",
      " [0.01158439]\n",
      " [0.01144625]\n",
      " [0.01167921]], shape=(4, 1), dtype=float32) b= tf.Tensor([0.97206706], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 9 227.89528\n",
      "W= tf.Tensor(\n",
      "[[0.01259636]\n",
      " [0.01261913]\n",
      " [0.01246865]\n",
      " [0.01272242]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.0588992], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 10 227.14842\n",
      "W= tf.Tensor(\n",
      "[[0.01360856]\n",
      " [0.01363316]\n",
      " [0.01347059]\n",
      " [0.01374474]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.1439936], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 11 226.43115\n",
      "W= tf.Tensor(\n",
      "[[0.01460049]\n",
      " [0.01462688]\n",
      " [0.01445246]\n",
      " [0.0147466 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.2273853], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 12 225.74232\n",
      "W= tf.Tensor(\n",
      "[[0.01557256]\n",
      " [0.01560071]\n",
      " [0.01541467]\n",
      " [0.0157284 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.3091081], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 13 225.08076\n",
      "W= tf.Tensor(\n",
      "[[0.01652516]\n",
      " [0.01655504]\n",
      " [0.01635762]\n",
      " [0.01669054]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.3891956], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 14 224.4454\n",
      "W= tf.Tensor(\n",
      "[[0.01745869]\n",
      " [0.01749025]\n",
      " [0.01728169]\n",
      " [0.01763341]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.4676803], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 15 223.83525\n",
      "W= tf.Tensor(\n",
      "[[0.01837353]\n",
      " [0.01840675]\n",
      " [0.01818725]\n",
      " [0.01855741]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.5445945], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 16 223.24927\n",
      "W= tf.Tensor(\n",
      "[[0.01927005]\n",
      " [0.01930489]\n",
      " [0.01907468]\n",
      " [0.0194629 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.6199696], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 17 222.68648\n",
      "W= tf.Tensor(\n",
      "[[0.02014862]\n",
      " [0.02018505]\n",
      " [0.01994435]\n",
      " [0.02035026]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.6938363], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 18 222.14601\n",
      "W= tf.Tensor(\n",
      "[[0.0210096 ]\n",
      " [0.02104758]\n",
      " [0.02079659]\n",
      " [0.02121986]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.7662249], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 19 221.62694\n",
      "W= tf.Tensor(\n",
      "[[0.02185334]\n",
      " [0.02189284]\n",
      " [0.02163178]\n",
      " [0.02207204]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.8371648], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 20 221.12846\n",
      "W= tf.Tensor(\n",
      "[[0.02268018]\n",
      " [0.02272118]\n",
      " [0.02245023]\n",
      " [0.02290715]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.9066851], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 21 220.64972\n",
      "W= tf.Tensor(\n",
      "[[0.02349046]\n",
      " [0.02353293]\n",
      " [0.0232523 ]\n",
      " [0.02372555]], shape=(4, 1), dtype=float32) b= tf.Tensor([1.9748143], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 22 220.18993\n",
      "W= tf.Tensor(\n",
      "[[0.02428452]\n",
      " [0.02432843]\n",
      " [0.02403831]\n",
      " [0.02452755]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.04158], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 23 219.74837\n",
      "W= tf.Tensor(\n",
      "[[0.02506268]\n",
      " [0.02510799]\n",
      " [0.02480858]\n",
      " [0.0253135 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.1070096], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 24 219.32433\n",
      "W= tf.Tensor(\n",
      "[[0.02582525]\n",
      " [0.02587194]\n",
      " [0.02556342]\n",
      " [0.02608371]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.17113], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 25 218.91705\n",
      "W= tf.Tensor(\n",
      "[[0.02657256]\n",
      " [0.0266206 ]\n",
      " [0.02630315]\n",
      " [0.02683849]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.233967], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 26 218.52594\n",
      "W= tf.Tensor(\n",
      "[[0.0273049 ]\n",
      " [0.02735426]\n",
      " [0.02702807]\n",
      " [0.02757816]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.2955468], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 27 218.1503\n",
      "W= tf.Tensor(\n",
      "[[0.02802257]\n",
      " [0.02807323]\n",
      " [0.02773846]\n",
      " [0.02830301]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.3558943], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 28 217.78955\n",
      "W= tf.Tensor(\n",
      "[[0.02872587]\n",
      " [0.0287778 ]\n",
      " [0.02843463]\n",
      " [0.02901335]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.4150343], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 29 217.44312\n",
      "W= tf.Tensor(\n",
      "[[0.02941508]\n",
      " [0.02946826]\n",
      " [0.02911686]\n",
      " [0.02970946]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.4729908], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 30 217.1104\n",
      "W= tf.Tensor(\n",
      "[[0.03009049]\n",
      " [0.0301449 ]\n",
      " [0.02978542]\n",
      " [0.03039164]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.5297873], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 31 216.79088\n",
      "W= tf.Tensor(\n",
      "[[0.03075238]\n",
      " [0.03080798]\n",
      " [0.0304406 ]\n",
      " [0.03106014]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.5854473], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 32 216.48398\n",
      "W= tf.Tensor(\n",
      "[[0.03140101]\n",
      " [0.03145778]\n",
      " [0.03108265]\n",
      " [0.03171527]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.6399934], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 33 216.18927\n",
      "W= tf.Tensor(\n",
      "[[0.03203665]\n",
      " [0.03209456]\n",
      " [0.03171184]\n",
      " [0.03235726]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.693448], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 34 215.9062\n",
      "W= tf.Tensor(\n",
      "[[0.03265955]\n",
      " [0.0327186 ]\n",
      " [0.03232843]\n",
      " [0.03298641]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.745833], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 35 215.63438\n",
      "W= tf.Tensor(\n",
      "[[0.03326998]\n",
      " [0.03333014]\n",
      " [0.03293267]\n",
      " [0.03360295]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.7971697], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 36 215.37334\n",
      "W= tf.Tensor(\n",
      "[[0.03386819]\n",
      " [0.03392942]\n",
      " [0.03352481]\n",
      " [0.03420714]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.847479], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 37 215.12262\n",
      "W= tf.Tensor(\n",
      "[[0.03445441]\n",
      " [0.0345167 ]\n",
      " [0.03410509]\n",
      " [0.03479923]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.8967817], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 38 214.88184\n",
      "W= tf.Tensor(\n",
      "[[0.03502889]\n",
      " [0.03509222]\n",
      " [0.03467374]\n",
      " [0.03537945]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.9450977], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 39 214.6506\n",
      "W= tf.Tensor(\n",
      "[[0.03559186]\n",
      " [0.03565621]\n",
      " [0.03523101]\n",
      " [0.03594806]], shape=(4, 1), dtype=float32) b= tf.Tensor([2.992447], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 40 214.42853\n",
      "W= tf.Tensor(\n",
      "[[0.03614356]\n",
      " [0.03620891]\n",
      " [0.03577711]\n",
      " [0.03650528]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.0388484], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 41 214.21529\n",
      "W= tf.Tensor(\n",
      "[[0.0366842 ]\n",
      " [0.03675053]\n",
      " [0.03631227]\n",
      " [0.03705134]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.0843215], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 42 214.01042\n",
      "W= tf.Tensor(\n",
      "[[0.03721402]\n",
      " [0.0372813 ]\n",
      " [0.03683672]\n",
      " [0.03758646]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.1288846], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 43 213.81372\n",
      "W= tf.Tensor(\n",
      "[[0.03773322]\n",
      " [0.03780144]\n",
      " [0.03735065]\n",
      " [0.03811086]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.172556], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 44 213.62482\n",
      "W= tf.Tensor(\n",
      "[[0.03824202]\n",
      " [0.03831116]\n",
      " [0.0378543 ]\n",
      " [0.03862475]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.2153533], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 45 213.44337\n",
      "W= tf.Tensor(\n",
      "[[0.03874063]\n",
      " [0.03881067]\n",
      " [0.03834785]\n",
      " [0.03912835]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.2572942], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 46 213.26912\n",
      "W= tf.Tensor(\n",
      "[[0.03922925]\n",
      " [0.03930018]\n",
      " [0.03883151]\n",
      " [0.03962186]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.2983959], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 47 213.10178\n",
      "W= tf.Tensor(\n",
      "[[0.03970808]\n",
      " [0.03977988]\n",
      " [0.03930549]\n",
      " [0.04010548]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.338675], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 48 212.9411\n",
      "W= tf.Tensor(\n",
      "[[0.04017732]\n",
      " [0.04024996]\n",
      " [0.03976997]\n",
      " [0.04057942]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.378148], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 49 212.78674\n",
      "W= tf.Tensor(\n",
      "[[0.04063715]\n",
      " [0.04071063]\n",
      " [0.04022514]\n",
      " [0.04104386]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.4168313], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 50 212.63853\n",
      "W= tf.Tensor(\n",
      "[[0.04108777]\n",
      " [0.04116207]\n",
      " [0.0406712 ]\n",
      " [0.04149899]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.4547403], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 51 212.49615\n",
      "W= tf.Tensor(\n",
      "[[0.04152937]\n",
      " [0.04160446]\n",
      " [0.04110832]\n",
      " [0.04194501]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.4918907], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 52 212.35945\n",
      "W= tf.Tensor(\n",
      "[[0.04196212]\n",
      " [0.04203799]\n",
      " [0.04153668]\n",
      " [0.04238209]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.5282977], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 53 212.22815\n",
      "W= tf.Tensor(\n",
      "[[0.04238619]\n",
      " [0.04246284]\n",
      " [0.04195645]\n",
      " [0.04281041]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.5639763], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 54 212.10205\n",
      "W= tf.Tensor(\n",
      "[[0.04280177]\n",
      " [0.04287917]\n",
      " [0.04236782]\n",
      " [0.04323015]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.5989408], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 55 211.98096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= tf.Tensor(\n",
      "[[0.04320902]\n",
      " [0.04328716]\n",
      " [0.04277094]\n",
      " [0.04364148]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.6332057], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 56 211.86465\n",
      "W= tf.Tensor(\n",
      "[[0.04360811]\n",
      " [0.04368697]\n",
      " [0.04316599]\n",
      " [0.04404457]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.6667848], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 57 211.75298\n",
      "W= tf.Tensor(\n",
      "[[0.04399921]\n",
      " [0.04407877]\n",
      " [0.04355311]\n",
      " [0.04443958]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.699692], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 58 211.64569\n",
      "W= tf.Tensor(\n",
      "[[0.04438247]\n",
      " [0.04446273]\n",
      " [0.04393248]\n",
      " [0.04482667]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.7319407], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 59 211.5427\n",
      "W= tf.Tensor(\n",
      "[[0.04475804]\n",
      " [0.04483898]\n",
      " [0.04430425]\n",
      " [0.04520601]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.763544], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 60 211.44377\n",
      "W= tf.Tensor(\n",
      "[[0.0451261 ]\n",
      " [0.04520769]\n",
      " [0.04466857]\n",
      " [0.04557774]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.794515], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 61 211.34874\n",
      "W= tf.Tensor(\n",
      "[[0.04548677]\n",
      " [0.04556902]\n",
      " [0.04502559]\n",
      " [0.04594203]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.824866], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 62 211.25749\n",
      "W= tf.Tensor(\n",
      "[[0.04584021]\n",
      " [0.04592311]\n",
      " [0.04537545]\n",
      " [0.04629901]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.85461], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 63 211.16988\n",
      "W= tf.Tensor(\n",
      "[[0.04618658]\n",
      " [0.04627009]\n",
      " [0.0457183 ]\n",
      " [0.04664884]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.8837585], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 64 211.08571\n",
      "W= tf.Tensor(\n",
      "[[0.046526  ]\n",
      " [0.04661013]\n",
      " [0.04605427]\n",
      " [0.04699165]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.9123237], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 65 211.00488\n",
      "W= tf.Tensor(\n",
      "[[0.04685861]\n",
      " [0.04694335]\n",
      " [0.04638352]\n",
      " [0.0473276 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.9403174], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 66 210.92726\n",
      "W= tf.Tensor(\n",
      "[[0.04718456]\n",
      " [0.04726989]\n",
      " [0.04670616]\n",
      " [0.04765681]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.9677508], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 67 210.85272\n",
      "W= tf.Tensor(\n",
      "[[0.04750397]\n",
      " [0.04758988]\n",
      " [0.04702234]\n",
      " [0.04797943]], shape=(4, 1), dtype=float32) b= tf.Tensor([3.9946353], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 68 210.7811\n",
      "W= tf.Tensor(\n",
      "[[0.04781699]\n",
      " [0.04790346]\n",
      " [0.04733218]\n",
      " [0.04829557]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.020982], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 69 210.71236\n",
      "W= tf.Tensor(\n",
      "[[0.04812372]\n",
      " [0.04821075]\n",
      " [0.0476358 ]\n",
      " [0.04860538]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.046801], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 70 210.64632\n",
      "W= tf.Tensor(\n",
      "[[0.04842431]\n",
      " [0.04851189]\n",
      " [0.04793334]\n",
      " [0.04890898]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.0721035], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 71 210.5829\n",
      "W= tf.Tensor(\n",
      "[[0.04871888]\n",
      " [0.04880698]\n",
      " [0.04822491]\n",
      " [0.04920649]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.0968995], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 72 210.522\n",
      "W= tf.Tensor(\n",
      "[[0.04900753]\n",
      " [0.04909616]\n",
      " [0.04851064]\n",
      " [0.04949804]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.1211996], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 73 210.46349\n",
      "W= tf.Tensor(\n",
      "[[0.0492904 ]\n",
      " [0.04937955]\n",
      " [0.04879064]\n",
      " [0.04978374]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.1450133], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 74 210.40732\n",
      "W= tf.Tensor(\n",
      "[[0.0495676 ]\n",
      " [0.04965724]\n",
      " [0.04906503]\n",
      " [0.05006372]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.1683507], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 75 210.3534\n",
      "W= tf.Tensor(\n",
      "[[0.04983924]\n",
      " [0.04992938]\n",
      " [0.04933392]\n",
      " [0.05033808]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.1912208], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 76 210.30157\n",
      "W= tf.Tensor(\n",
      "[[0.05010544]\n",
      " [0.05019605]\n",
      " [0.04959741]\n",
      " [0.05060694]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.2136335], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 77 210.2518\n",
      "W= tf.Tensor(\n",
      "[[0.05036629]\n",
      " [0.05045738]\n",
      " [0.04985562]\n",
      " [0.0508704 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.2355976], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 78 210.20401\n",
      "W= tf.Tensor(\n",
      "[[0.05062192]\n",
      " [0.05071347]\n",
      " [0.05010865]\n",
      " [0.05112859]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.257122], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 79 210.15813\n",
      "W= tf.Tensor(\n",
      "[[0.05087241]\n",
      " [0.05096442]\n",
      " [0.05035661]\n",
      " [0.0513816 ]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.278216], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 80 210.11406\n",
      "W= tf.Tensor(\n",
      "[[0.05111789]\n",
      " [0.05121034]\n",
      " [0.0505996 ]\n",
      " [0.05162953]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.2988877], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 81 210.07175\n",
      "W= tf.Tensor(\n",
      "[[0.05135844]\n",
      " [0.05145132]\n",
      " [0.05083771]\n",
      " [0.05187249]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.3191457], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 82 210.03107\n",
      "W= tf.Tensor(\n",
      "[[0.05159416]\n",
      " [0.05168748]\n",
      " [0.05107104]\n",
      " [0.05211058]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.3389983], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 83 209.99203\n",
      "W= tf.Tensor(\n",
      "[[0.05182516]\n",
      " [0.05191889]\n",
      " [0.05129969]\n",
      " [0.05234389]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.3584538], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 84 209.95454\n",
      "W= tf.Tensor(\n",
      "[[0.05205152]\n",
      " [0.05214566]\n",
      " [0.05152376]\n",
      " [0.05257252]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.3775196], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 85 209.91853\n",
      "W= tf.Tensor(\n",
      "[[0.05227334]\n",
      " [0.05236789]\n",
      " [0.05174334]\n",
      " [0.05279656]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.396204], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 86 209.88396\n",
      "W= tf.Tensor(\n",
      "[[0.05249072]\n",
      " [0.05258566]\n",
      " [0.05195851]\n",
      " [0.05301611]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.4145145], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 87 209.85075\n",
      "W= tf.Tensor(\n",
      "[[0.05270373]\n",
      " [0.05279906]\n",
      " [0.05216936]\n",
      " [0.05323126]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.432459], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 88 209.81885\n",
      "W= tf.Tensor(\n",
      "[[0.05291247]\n",
      " [0.05300818]\n",
      " [0.05237598]\n",
      " [0.05344209]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.450044], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 89 209.78822\n",
      "W= tf.Tensor(\n",
      "[[0.05311702]\n",
      " [0.0532131 ]\n",
      " [0.05257846]\n",
      " [0.05364869]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.4672775], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 90 209.75879\n",
      "W= tf.Tensor(\n",
      "[[0.05331747]\n",
      " [0.05341391]\n",
      " [0.05277687]\n",
      " [0.05385115]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.4841657], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 91 209.73055\n",
      "W= tf.Tensor(\n",
      "[[0.05351389]\n",
      " [0.05361069]\n",
      " [0.0529713 ]\n",
      " [0.05404954]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.500716], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 92 209.70341\n",
      "W= tf.Tensor(\n",
      "[[0.05370637]\n",
      " [0.05380352]\n",
      " [0.05316183]\n",
      " [0.05424395]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.5169353], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 93 209.67735\n",
      "W= tf.Tensor(\n",
      "[[0.05389499]\n",
      " [0.05399248]\n",
      " [0.05334854]\n",
      " [0.05443446]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.5328298], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 94 209.65231\n",
      "W= tf.Tensor(\n",
      "[[0.05407983]\n",
      " [0.05417765]\n",
      " [0.0535315 ]\n",
      " [0.05462114]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.548406], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 95 209.62831\n",
      "W= tf.Tensor(\n",
      "[[0.05426095]\n",
      " [0.0543591 ]\n",
      " [0.05371078]\n",
      " [0.05480408]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.563671], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 96 209.60521\n",
      "W= tf.Tensor(\n",
      "[[0.05443844]\n",
      " [0.05453691]\n",
      " [0.05388647]\n",
      " [0.05498335]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.5786304], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 97 209.58304\n",
      "W= tf.Tensor(\n",
      "[[0.05461236]\n",
      " [0.05471116]\n",
      " [0.05405863]\n",
      " [0.05515902]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.5932903], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 98 209.56177\n",
      "W= tf.Tensor(\n",
      "[[0.0547828 ]\n",
      " [0.0548819 ]\n",
      " [0.05422734]\n",
      " [0.05533116]], shape=(4, 1), dtype=float32) b= tf.Tensor([4.607657], shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 99 209.5413\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    W, b = train(X_arr, y_arr, W, b)\n",
    "    print('W=',W,'b=',b)\n",
    "    print('Current Loss on iteration', i, loss(y_arr, prediction(X_arr, W, b)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vanvD93FV0_k"
   },
   "source": [
    "### Observe values of Weight\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSqpy4gtWaOD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight= tf.Tensor(\n",
      "[[0.0547828 ]\n",
      " [0.0548819 ]\n",
      " [0.05422734]\n",
      " [0.05533116]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Weight=',W )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9KpRupYUEwy"
   },
   "source": [
    "### Observe values of Bias\n",
    "- Print the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhEWkGqHWohg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias= tf.Tensor([4.607657], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Bias=',b )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Internal - R6 - AIML Labs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
